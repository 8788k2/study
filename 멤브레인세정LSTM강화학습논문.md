# Deep reinforcement learning in an ultrafiltration system: Optimizin operating pressure and chemical cleaning conditions
LSTM을 통해 멤브레인 파울링 클리닝 시스템의 동적 특성을 학습하고 이를 강화학습의 환경으로 적용한 논문의 내용을 공부하는 것을 목표로한다. (IF 8.1)

이는 수행중인 기업과제의 구조와 매우 유사하며, 환경공학적 분야에 머신러닝을 활용한 사례로서 본인의 연구 주제와 그 실현 가능성을 탐색하는데 큰 도움이 될것으로 기대된다. 

## 실험 환경 및 조건(학습 데이터셋 확보)
PVDF 울트라필터레이션 모듈 사용

인천 해안에서 체취한 원수에 오염물질 첨가하여 모듈에 주입 -> 파울링 유도

한번의 실험마다 총 14번의 Sub-filtration을 세정 과정 사이에 수행

각 서브필터레이션 수행 30분 후 마다 세정을 수행

세정은 약품(NaOH)세정을 먼저 수행하고 두번의 백워시를 각 55초동안 진행 

약품세정 조건은 데이터 셋 얻기위해 바꿔가며 진행 (세정시간 20, 30, 35, 40, 55, 75 s/ 농도 100, 150, 200, 300, 650 ppm) 

총 11번의 실험을 약품세정조건을 바꿔가며 진행 (기본 세정시간은 35초, 기본 약품농도는 650)

### 결론

```
각 서브필터레이션 30분 후 약품세정 → 백워시 2회" 구조는 고정되어 있으며, 그 세정 단계에서 사용된 NaOH 농도는 0.1M이 아닌 5개의 희석된 농도 조건으로 바꿔가며 실험을 수행했고, 세정 시간도 6가지로 나눠서 조합했다
```

## LSTM을 통한 트랜지션 펑션 학습
### 비판

30분 후의 플럭스를 에측 (모든 다음 상태가 아웃풋은 아님)

즉 다음 스테이트를 축소하여 학습함 --> 한계점 존재

다음 한스텝까지의 스테이트만 이용하여 보상함수를 설계 즉, step local optimal

---
그럼에도 불구하고 학습을 해두고 나면 정책 주어지므로 계산코스트 필요 없이 실시간 대응 가능


## DRL 구조

액션과 상태 한 스텝 즉 dt = 30분
30분마다 최적의 세정 시간 약품농도 압력을 결정하여 파울링 관리




## 주요 성과
```
평균 flux 10퍼센트 증가
압력 12.5퍼센트 감소
약품농도 4.8퍼센트 감소
에너지소모 20.9퍼센트절감
```

약품농도는 줄이고 세정시간을 늘리는 전략 채택


## 생각포인트 
전이함수 유도를 위해 LSTM을 학습시킬 때 제어입력, 액션에 해당하는 파라미터는 다양성을 확보할 수 있지만 나머지 9개 가량의 스테이트에 해당하는 파라미터의 다양성은 어떻게 확보할 것인가

전이함수를 유도할 때 최대한 다양한 분포의 스테이트를 학습해야 강화학습 정책이 서브옵티멀하지 않고 강건해질 수 있다. 

11개의 실험 조건마다 조금 다르게 했나?? 

--> 데이터 뽑을 실험에서는 압력을 바꾸지 않았지만 DRL학습에선 액션으로 줌


DRL이 다양한 시나리오 학습한다고 해도, 그 시나리오가 LSTM이 표현가능한 범위(학습 데이터 범위, 인풋파라미터) 내에서만 생성되므로 학습 데이터 매우 중요





예상 질문 리스트업

1. 왜 모델프리 안하고 굳이 학습을 통해 전이함수 모덿 만들어??

2. 이런 분야에선 전문가 데이터가 있을텐데 더 나은 점이 뭐야

전문가 데이터: 한 작업에 대하여 높은 성공률을 가지는 정책을 기반으로 생성된 상태와 행동 데이터

정확한 질문입니다.  
강화학습(DRL)은 실제로 **한 스텝에서만 보상을 받는 구조(1-step prediction 환경)**에서 학습을 하더라도, 이론적으로는 **장기적 최적화(long-term optimization)**을 수행할 수 있습니다.

그 핵심은 바로 **discounted cumulative reward** 구조에 있습니다.

---

## ✅ 핵심 개념: 강화학습은 기본적으로 "누적 보상"을 최적화한다

강화학습의 학습 목표는 아래와 같은 **누적 보상 함수(return)**를 최대화하는 policy \( \pi \)를 찾는 것입니다:

\[
\boxed{
R_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k}
}
\]

여기서:

- \( R_t \): 시점 \( t \)에서의 총 누적 보상 (return)
- \( r_{t+k} \): \( t+k \) 시점에서 받은 보상
- \( \gamma \in [0,1) \): discount factor (보상의 미래 가치 감쇠율)

---

## 🎯 그럼 중요한 질문:

> ❓ "환경이 다음 스텝만 예측하고, 보상도 그 한 스텝에 대해서만 계산하는데, 어떻게 장기 최적화를 학습한다는 건가요?"

→ **이건 매우 중요하고, 답은 다음과 같습니다:**

---

## ✅ A. DRL은 다음 상태를 추정하고, 그 상태에서 또 다음 액션을 취할 것까지 고려함

즉, DRL은 학습 과정에서 다음과 같은 value function을 내재적으로 학습합니다:

\[
V^\pi(s_t) = \mathbb{E} \left[ \sum_{k=0}^{\infty} \gamma^k r_{t+k} \mid s_t \right]
\]

→ 이 말은, **현재 상태 \( s_t \)에서 어떤 액션 \( a_t \)를 취하면, 그 다음 상태 \( s_{t+1} \)에서 어떤 보상이 누적적으로 따라올지를 예측하는 구조**입니다.

> 💡 즉, 비록 한 번에 하나의 보상만 계산하더라도, **그 이후 상태에서 어떤 보상이 누적적으로 발생할지를 DRL은 학습한다.**

---

## ✅ B. Value function이 이 역할을 수행

- Actor-Critic 기반 알고리즘 (PPO 포함)은 **policy**와 **value function \( V^\pi(s) \)**를 함께 학습
- 이 value function은 **"앞으로 받을 총 보상에 대한 기대값"**을 학습하므로,
  - 당장의 reward가 적더라도
  - **장기적으로 이득이 더 큰 상태를 선택하는 방향으로 policy가 조정됨**

---

## ✅ C. 예시로 이해: 세정 주기를 최적화하는 상황

- 초반에 세정을 너무 세게 하면 flux는 높아짐 → 당장 보상 좋음
- BUT 약품/에너지 비용 증가 → **장기 누적 보상은 나빠질 수 있음**

→ DRL은 이런 trade-off를 수천 회의 episode를 통해 경험하면서  
**“지금은 조금 덜 세정하고, 나중에 더 강하게 세정하는 게 좋겠구나”** 같은 **전략(policy)**을 학습

---

## ✅ D. 왜 LSTM 기반 단일 스텝 예측 모델에서도 가능하냐?

- 비록 환경 모델이 한 스텝만 예측하더라도,
- DRL 에이전트는 **그 다음 상태 \( s_{t+1} \)**에서 또 액션을 취하고, 또 보상을 받을 것을 가정하며 학습
- 이 과정을 수천 회 반복하여, 결국 “지금 행동이 미래까지 어떤 영향을 미치는지”를 정책과 value function에 반영

---

## 🧠 요약: 단일 스텝 환경에서도 DRL이 장기 최적화를 학습하는 이유

| 요소 | 설명 |
|------|------|
| 🧮 Discounted return | DRL의 학습 목표 자체가 누적 보상 \( R_t = \sum \gamma^k r_{t+k} \) |
| 🔁 반복적 상호작용 | 매 스텝마다 상태 → 액션 → 보상 → 다음 상태로 넘어가며 학습 |
| 📈 Value function | 장기적 가치 \( V(s_t) \)를 예측하여 “지금 행동의 미래 영향” 반영 |
| 🔍 구조적 특징 | DRL은 마치 환경이 multi-step인 것처럼 추론하며 학습할 수 있음 (rollout 방식)

---

## 🎯 결론

> ❗ 비록 환경 모델이 단일 스텝 예측만 하더라도,  
> **DRL은 누적 보상을 최적화하기 때문에 결과적으로 장기 전략을 학습할 수 있습니다.**

---

필요하시면 이 구조를 수학적 모델, 코드 예시(PPO or DQN), 시각적 개념도 등으로 확장해서 설명드릴 수 있어요.  
다음 단계로 이어가실까요?