# Deep reinforcement learning in an ultrafiltration system: Optimizing operating pressure and chemical cleaning conditions
LSTM을 통해 멤브레인 파울링 클리닝 시스템의 동적 특성을 학습하고 이를 강화학습의 환경으로 적용한 논문의 내용을 공부하는 것을 목표로한다. (IF 8.1)

이는 수행중인 기업과제의 구조와 매우 유사하며, 환경공학적 분야에 머신러닝을 활용한 사례로서 본인의 연구 주제와 그 실현 가능성을 탐색하는데 큰 도움이 될것으로 기대된다. 

## 실험 환경 및 조건(학습 데이터셋 확보)
PVDF 울트라필터레이션 모듈 사용

인천 해안에서 체취한 원수에 오염물질 첨가하여 모듈에 주입 -> 파울링 유도

한번의 실험마다 총 14번의 Sub-filtration을 세정 과정 사이에 수행

각 서브필터레이션 수행 30분 후 마다 세정을 수행

세정은 약품(NaOH)세정을 먼저 수행하고 두번의 백워시를 각 55초동안 진행 

약품세정 조건은 데이터 셋 얻기위해 바꿔가며 진행 (세정시간 20, 30, 35, 40, 55, 75 s/ 농도 100, 150, 200, 300, 650 ppm) 

총 11번의 실험을 약품세정조건을 바꿔가며 진행 (기본 세정시간은 35초, 기본 약품농도는 650)

### 결론

```
각 서브필터레이션 30분 후 약품세정 → 백워시 2회" 구조는 고정되어 있으며, 그 세정 단계에서 사용된 NaOH 농도는 0.1M이 아닌 5개의 희석된 농도 조건으로 바꿔가며 실험을 수행했고, 세정 시간도 6가지로 나눠서 조합했다
```

## LSTM을 통한 트랜지션 펑션 학습
인풋 파라미터
```
누적 시간 (cumulative time)

초기 flux (initial water flux)

압력 (pressure)

유입 탁도 (inlet turbidity)

유출 탁도 (outlet turbidity)

유입 전도도 (inlet conductivity)

유출 전도도 (outlet conductivity)

수온 (temperature)

세정 방식 (type of cleaning)

백워시 시간 (backwashing time)

화학세정 시간 (chemical cleaning time)

화학세정 농도 (chemical cleaning concentration)
```





### 비판

30분 후의 플럭스를 에측 (모든 다음 상태가 아웃풋은 아님)

즉 다음 스테이트를 축소하여 학습함 --> 한계점 존재

다음 한스텝까지의 스테이트만 이용하여 보상함수를 설계 즉, step local optimal

---
그럼에도 불구하고 학습을 해두고 나면 정책 주어지므로 계산코스트 필요 없이 실시간 대응 가능


## DRL 구조

액션과 상태 한 스텝 즉 dt = 30분
30분마다 최적의 세정 시간 약품농도 압력을 결정하여 파울링 관리

사용한 알고리즘: PPO(Proximal Policy Optimiaztion) 

Policy network: multi-layer perceptron 은닉층 2개, 각 층에 64개의 뉴런 구성
입력은 현재 상태, 출력은 가장 높은 보상을 약속하는 액션 

water flux는 액션이 적용되면 LSTM 기반 환경으로부터 획득되는 상태 

액션: 운영압력 0.5~0.7 bar, 세정시간 24~45 s, 세정 약품 농도: 450~650 ppm

### Reward 구조

DRL이 정책에 따라 현재상태에서 최적의 액션을 선택, 그 액션과 상태를 베이스로 LSTM이 예측한 FLUX = $J_{controlled}$ 

이전 DRL이 산출한 최적의 액션이 아니라 기존 운영 전력에서 사용했던 액션과 상태를 베이스로 LSTM이 예측한 FLUX = $J_{actural}$

$$
R_{\text{positive}} = f_1 \cdot (J_{\text{controlled}} - J_{\text{actual}})
$$

$f$는 가중치

$R_{\text{negative}}$는 너무 큰 액션값을 주는것을 방지하기 위한 패널티 항 

$$
R_{\text{negative}} = \sum_i f_i \cdot |A_{i,\text{controlled}} - A_{i,\text{actual}}|
$$


최정적으로 논문에서 사용한 보상함수 구조는 아래와 같다
$$
R = R_{\text{positive}} - R_{\text{negative}}
$$ 




## 주요 성과
```
평균 flux 10퍼센트 증가
압력 12.5퍼센트 감소
약품농도 4.8퍼센트 감소
에너지소모 20.9퍼센트절감
```

약품농도는 줄이고 세정시간을 늘리는 전략 채택


## 생각포인트 
전이함수 유도를 위해 LSTM을 학습시킬 때 제어입력, 액션에 해당하는 파라미터는 다양성을 확보할 수 있지만 나머지 9개 가량의 스테이트에 해당하는 파라미터의 다양성은 어떻게 확보할 것인가

전이함수를 유도할 때 최대한 다양한 분포의 스테이트를 학습해야 강화학습 정책이 서브옵티멀하지 않고 강건해질 수 있다. 

11개의 실험 조건마다 조금 다르게 했나?? 

--> 데이터 뽑을 실험에서는 압력을 바꾸지 않았지만 DRL학습에선 액션으로 줌


DRL이 다양한 시나리오 학습한다고 해도, 그 시나리오가 LSTM이 표현가능한 범위(학습 데이터 범위, 인풋파라미터) 내에서만 생성되므로 학습 데이터 매우 중요


---
누적보상 계산에서 t+1 시점에서의 정책함수를 통해 액션을 산출할 때 인풋의 차원문제를 어떻게 해결했는지?

서브옵티멀, 왜곡현상 발생할 수 있는것 아닌가??

이 문제 해결을 다음 논문의 핵심 컨트리뷰션으로 삼고 기업과제에도 적용해보면 어때?? 

스테이트 계층화를 통해 꼭 예측해서 업데이트 해줘야하는 스테이트와 외부 주입이나 상수처리해도 되거나, 정책이 업데이트해야할 스테이트를 구분



---
시간어텐션(연관성 깊은 시계열 포인트 학습), 입력어텐션(입력 가운데 중요한 피쳐 학습) 구조 도임으로 정확도 향상?? -> 핵심은 전이함수가 최대한 다양한 스테이트를 아웃풋으로 산출하면서도 높은 정확도 갖는것





예상 질문 리스트업

1. 왜 모델프리 안하고 굳이 학습을 통해 전이함수 모델 왜 만들어??

2. 이런 분야에선 전문가 데이터가 있을텐데 더 나은 점이 뭐야

전문가 데이터: 한 작업에 대하여 높은 성공률을 가지는 정책을 기반으로 생성된 상태와 행동 데이터

## 추가논문: Improved Long Short-Term Memory-based Wastewater Treatment Simulators for Deep Reinforcement Learning
LSTM으로 강화학습 환경 구축에 활용을 목적으로 하는 논문

### 실험환경 및 데이터셋
덴마크 하수처리장의 2년간 SCADA 실측 데이터

데이터 전처리로 상관관계 분석을 통해 상태변수, 제어입력 정의

| 인풋                | 설명                                      | 비고                                  |
|---------------------------|------------------------------------------|---------------------------------------|
| **Phosphate (PO₄³⁻)**     | 목표 출력 변수 — DRL의 보상에 사용됨       | [mg/L]                                |
| Metal dosage (previous)  | 직전 타임스텝의 제어값                     | recurrent input                       |
| Reactor inflow rate      | 반응조 유입 유량                          | [m³/h]                                |
| pH                        | 반응조 내 pH                              |                                       |
| TSS                      | 부유물 농도                               | Total Suspended Solids               |
| NH₄⁺                    | 암모늄 농도                               | 질소성 물질                           |
| DO                        | Dissolved Oxygen (용존산소)               | 반응조 내                             |
| ORP                       | 산화환원 전위                             | [mV]                                  |
| 온도 (Temperature)        | 반응조 내부 온도                          | [°C]                                  |
| 시간 인덱스 (timestep)     | 주기, 계절성 고려                         | 일/시간 정보 포함                    |
| …                        | + 기타 SCADA 기반 시간정보 (예: 요일 등) | 일부 inferred                         |

### 알고리즘 구조와 컨트리뷰션
LSTM 2층, 각 256 유닛, 활성화함수 thah, 드롭아웃 0.15

과거 상태변수와 입력을 안풋으로 받아서 다음 상태 예측

성능 향상을 위해 두가지 핵심 기법을 적용

**DaD (Data as Demonstrator)**: 모델이 예측한 값을 입력으로 다시 사용하는 self-correction 반복학습 -> 강화학습의 rollout 누적 보상 계산에도 강건하게 작동할 수 있도록함

**DILATE (DIstortion Loss including shApe and Time) loss**: 예측값과 실제값의 형태적 유사성(shape)과 시간축 일치성을 모두 고려하도록 DTW 기반 코스트펑션 설계 


$$
\mathcal{L}_{\text{DILATE}} = \alpha \cdot \mathcal{L}_{\text{shape}} + (1 - \alpha) \cdot \mathcal{L}_{\text{time}}
$$


$$
\mathcal{L}_{\text{shape}} = \frac{1}{T} \sum_{t=1}^{T} (y_t - \hat{y}_t)^2
$$
**1부터 T까지의 $y_t$ LSTM이 recursive하게 자기자신의 예측값을 다시 입력으로 사용하여 얻은 값 (Recursive Multi step Forecasting)**

**상태는 예측된 값으로 업데이트하고 입력은 실제 운전시나리오에서 선택한 값으로 업데이트**

얼마나 LSTM이 리컬시브하게 계산해도 원래 데이터와 Shape가 유사한지를 나타내는 척도

$$
\mathcal{L}_{\text{time}} = \text{DTW}(y, \hat{y})
$$
DTW(Dynamic Time Warping)은 파이썬이나 매틀랩 라이브러리로 제공됨
두 시계열 데이터 간 오차합이 최소가 되는 최적 정렬을 다이내믹 프로그래밍으로 찾아주는 알고리즘

실제 시계열 값이 0 0 1 1 2 3 4 5,

예측 시계열 값이 0 0 1 2 3 4 5 5, 라면 단순 MSE를 계산하면 오차가 큰 것으로 인식되지만

DTW는 시퀀스를 시점 단위로 1:1 비교를 하는게 아니라 적당히 늘리고 당겨서 정렬하여 유사한 shape을 따져서 오차를 계산

여기서 시계열 정렬은 시간 역행하여 매칭할 수 없고 같은 시계열을 중복 적용시키거나 다음 시계열을 매칭시켜야 함 즉, 단조 조건

-> **단순히 지연되거나 빠른 예측에는 큰 패널티를 부과하지 않아 패턴과 흐름을 예측하는 방향으로 LSTM 모델을 학습 시켜 강화학습에 적절하게 사용 가능**

### 학습 전략
시계열 길이와 시작 시점을 다르게 4가지 에피소드 구성하여 LSTM의 성능을 비교

각 에피소드에 대해 학습 -> 같은 데이터셋으로 장기 예측 테스트까지 진행
```
E1 고정 길이, 시작 지점

E2 랜덤 길이, 고정 시작 지점

E3 고정 길이, 랜덤 시작 지점

E4 랜덤 길이, 시작 지점 -> 현실 상황 모사 목적
```

에피소드 길이는 최대 2880, 최소 10

E4로 학습 시킨 모델이 월별 평균 MSE, DTW 모두 가장 우수

베이스라인 LSTM 대비 큰 성능 개선

