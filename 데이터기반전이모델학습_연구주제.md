# 데이터 기반 전이모델 학습 및 제어 환경 구축
비선형·타임바링 시스템(HVAC, 수처리 플랜트 등)에서의 물리 모델 부재 문제를, 데이터 기반 전이 모델로 해결하고, 이를 강화학습 또는 고전제어(MPC 등)에 활용한다.

이를 통해 사용자의 직접 조작, 전문가 데이터 기반 운영에서 벗어나 지능화된 자율운영 시스템 구축을 목표로 한다.

## Concept of Project
![concept](images/concept%20of%20LG%20project.png)


수행하고 있는 기업 과제의 컨셉과 개인 연구 주제 연결된다면 좋지 않을까? 

구상하는 컨셉과 관련된 논문이 있는지 찾아본 결과 LSTM을 통해 멤브레인 파울링 클리닝 시스템, 하수처리 플랜트의 동적 특성을 학습하고 이를 강화학습의 환경으로 적용한 논문 2편 존재하여 공부

이는 수행중인 기업과제의 구조와 매우 유사하며, 환경공학 분야에 머신러닝을 활용한 사례로서 연구 주제와 그 실현 가능성을 탐색하는데 도움이 될것으로 기대된다. 



## Deep reinforcement learning in an ultrafiltration system: Optimizing operating pressure and chemical cleaning conditions
멤브레인 막여과 시스템을 LSTM으로 학습하고 DRL에 적용한 사례

## 실험 환경 및 조건(학습 데이터셋 확보)
PVDF 울트라필터레이션 모듈 사용

인천 해안에서 체취한 원수에 오염물질 첨가하여 모듈에 주입 -> 파울링 유도

한번의 실험마다 총 14번의 Sub-filtration을 세정 과정 사이에 수행

각 서브필터레이션 수행 30분 후 마다 세정을 수행

세정은 약품(NaOH)세정을 먼저 수행하고 두번의 백워시를 각 55초동안 진행 

약품세정 조건은 데이터 셋 얻기위해 바꿔가며 진행 (세정시간 20, 30, 35, 40, 55, 75 s/ 농도 100, 150, 200, 300, 650 ppm) 

총 11번의 실험을 약품세정조건을 바꿔가며 진행 (기본 세정시간은 35초, 기본 약품농도는 650)

```
각 서브필터레이션 30분 후 약품세정 → 백워시 2회" 구조는 고정되어 있으며, 그 세정 단계에서 사용된 NaOH 농도는 5개의 희석된 농도 조건으로 바꿔가며 실험을 수행했고, 세정 시간도 6가지로 나눠서 조합했다
```

## LSTM을 통한 전이모델
dt = 30min, 한 스텝은 30분

인풋: 현재 시점의 feature들

| feature                           | 설명                                              | 비고                        |
|--------------------------------|---------------------------------------------------|-----------------------------|
| Cumulative time                | 누적 운전 시간                                     | [min]     |
| Initial water flux            | 초기 시점의 유속                                   | [L/m²·h] 등                 |
| Pressure                      | 운영 압력                                         | [bar]                       |
| Inlet turbidity               | 유입수의 탁도                                     | [NTU]                       |
| Outlet turbidity              | 유출수의 탁도                                     | [NTU]                       |
| Inlet conductivity            | 유입수의 전도도                                   | [μS/cm]                     |
| Outlet conductivity           | 유출수의 전도도                                   | [μS/cm]                     |
| Temperature                   | 수온                                              | [°C]                        |
| Type of cleaning              | 세정 방식 구분                                    | categorical |
| Backwashing time              | 백워시(역세척) 지속 시간                           | [sec]                       |
| Chemical cleaning time        | 화학 세정 시간 (제어 입력)                        | [sec]                       |
| Chemical cleaning concentration | 화학 세정 농도 (제어 입력)                         | [ppm]                       |

아웃풋: 30분 후의 FLUX

---
히든스테이트 차원: 64

$R^2$ = 0.93

### 비판

30분 후의 플럭스를 에측 (모든 다음 상태가 아웃풋은 아님)

즉 다음 스테이트를 축소하여 학습함 --> 한계점 존재

다음 한스텝까지의 스테이트만 이용하여 보상함수를 설계 즉, step local optimal


## DRL 구조

액션과 상태 한 스텝 즉 dt = 30분
30분마다 최적의 세정 시간 약품농도 압력을 결정하여 파울링 관리

사용한 알고리즘: PPO(Proximal Policy Optimiaztion) 

Policy network: multi-layer perceptron 은닉층 2개, 각 층에 64개의 뉴런 구성
입력은 현재 상태, 출력은 가장 높은 보상을 약속하는 액션 

water flux는 액션이 적용되면 LSTM 기반 환경으로부터 획득되는 상태 

액션: 운영압력 0.5~0.7 bar, 세정시간 24~45 s, 세정 약품 농도: 450~650 ppm

### Reward 구조

DRL이 정책에 따라 현재상태에서 최적의 액션을 선택, 그 액션과 상태를 베이스로 LSTM이 예측한 FLUX = $J_{controlled}$ 

이전 DRL이 산출한 최적의 액션이 아니라 기존 운영 전력에서 사용했던 액션과 상태를 베이스로 LSTM이 예측한 FLUX = $J_{actural}$

$$
R_{\text{positive}} = f_1 \cdot (J_{\text{controlled}} - J_{\text{actual}})
$$

$f$는 가중치

$R_{\text{negative}}$는 너무 큰 액션값을 주는것을 방지하기 위한 패널티 항 

$$
R_{\text{negative}} = \sum_i f_i \cdot |A_{i,\text{controlled}} - A_{i,\text{actual}}|
$$


최정적으로 논문에서 사용한 보상함수 구조는 아래와 같다
$$
R = R_{\text{positive}} - R_{\text{negative}}
$$ 




## 주요 성과
```
평균 flux 10퍼센트 증가
압력 12.5퍼센트 감소
약품농도 4.8퍼센트 감소
에너지소모 20.9퍼센트절감
```


## Improved Long Short-Term Memory-based Wastewater Treatment Simulators for Deep Reinforcement Learning
LSTM으로 하수처리 플랜트의 전이모델을 학습하여 강화학습 환경 구축에 활용을 목적으로 하는 논문

### 실험환경 및 데이터셋
덴마크 하수처리장의 2년간 SCADA 실측 데이터(각 월별로 데이터 정리되어 있어 계절에 따른 서브옵티멀 문제 해소)

인농도를 메인 타겟으로 잡고 제어가능한 액션은 알루미늄 등 금속 약품 투여량

### LSTM 구조
인풋 과거 시점 $l$개의 feature들  

| feature                | 설명                                      | 비고                                  |
|---------------------------|------------------------------------------|---------------------------------------|
| **Phosphate (PO₄³⁻)**     | 목표 출력 변수 — DRL의 보상에 사용됨       | [mg/L]                                |
| Metal dosage (previous)  | 제어값 즉 Action                    | recurrent input                       |
| Reactor inflow rate      | 반응조 유입 유량                          | [m³/h]                                |
| pH                        | 반응조 내 pH                              |                                       |
| TSS                      | 부유물 농도                               | Total Suspended Solids               |
| NH₄⁺                    | 암모늄 농도                               | 질소성 물질                           |
| DO                        | Dissolved Oxygen (용존산소)               | 반응조 내                             |
| ORP                       | 산화환원 전위                             | [mV]                                  |
| 온도 (Temperature)        | 반응조 내부 온도                          | [°C]                                  |
| 시간 인덱스 (timestep)     | 주기, 계절성 고려                         | 일/시간 정보 포함                    |
| …                        | + 기타 SCADA 기반 시간정보 (예: 요일 등) | 일부 inferred                         |

아웃풋: feature에서 제어값을 제외한 다음 한 스텝의 상태 즉 $\hat{x}(t+1)$



### 알고리즘 구조와 컨트리뷰션
LSTM 2층, 각 256 유닛, 활성화함수 thah, 드롭아웃 0.15

과거 상태변수와 입력을 안풋으로 받아서 다음 상태 예측

성능 향상을 위해 두가지 핵심 기법을 적용

**DaD (Data as Demonstrator)**: 모델이 예측한 값을 입력으로 다시 사용하는 self-correction 반복학습 -> 강화학습의 rollout 누적 보상 계산에도 강건하게 작동할 수 있도록함

**DILATE (DIstortion Loss including shApe and Time) loss**: 예측값과 실제값의 형태적 유사성(shape)과 시간축 일치성을 모두 고려하도록 DTW 기반 코스트펑션 설계 


### LOSS function
$$
\mathcal{L}_{\text{DILATE}} = \alpha \cdot \mathcal{L}_{\text{shape}} + (1 - \alpha) \cdot \mathcal{L}_{\text{time}}
$$


$$
\mathcal{L}_{\text{shape}} = \frac{1}{T} \sum_{t=1}^{T} (y_t - \hat{y}_t)^2
$$
**1부터 T까지의 $y_t$ LSTM이 recursive하게 자기자신의 예측값을 다시 입력으로 사용하여 얻은 값 (Recursive Multi step Forecasting)**

**상태는 예측된 값으로 업데이트하고 입력은 실제 운전시나리오에서 선택한 값으로 업데이트**

얼마나 LSTM이 리컬시브하게 계산해도 원래 데이터와 Shape가 유사한지를 나타내는 척도

$$
\mathcal{L}_{\text{time}} = \text{DTW}(y, \hat{y})
$$
DTW(Dynamic Time Warping)은 파이썬이나 매틀랩 라이브러리로 제공됨
두 시계열 데이터 간 오차합이 최소가 되는 최적 정렬을 다이내믹 프로그래밍으로 찾아주는 알고리즘

실제 시계열 값이 0 0 1 1 2 3 4 5,

예측 시계열 값이 0 0 1 2 3 4 5 5, 라면 단순 MSE를 계산하면 오차가 큰 것으로 인식되지만

DTW는 시퀀스를 시점 단위로 1:1 비교를 하는게 아니라 적당히 늘리고 당겨서 정렬하여 유사한 shape을 따져서 오차를 계산

여기서 시계열 정렬은 시간 역행하여 매칭할 수 없고 같은 시계열을 중복 적용시키거나 다음 시계열을 매칭시켜야 함 즉, 단조 조건

-> **단순히 지연되거나 빠른 예측에는 큰 패널티를 부과하지 않아 패턴과 흐름을 예측하는 방향으로 LSTM 모델을 학습 시켜 강화학습에 적절하게 사용 가능**

### 학습 전략
시계열 길이와 시작 시점을 다르게 4가지 에피소드 구성하여 LSTM의 성능을 비교

각 에피소드에 대해 학습 -> 같은 데이터셋으로 장기 예측 테스트까지 진행
```
E1 고정 길이, 시작 지점

E2 랜덤 길이, 고정 시작 지점

E3 고정 길이, 랜덤 시작 지점

E4 랜덤 길이, 시작 지점 -> 현실 상황 모사 목적
```

에피소드 길이는 최대 2880, 최소 10



### 주요 성과

```
E4로 학습 시킨 모델이 월별 평균 MSE, DTW 모두 가장 우수
베이스라인 LSTM 대비 큰 성능 개선하여 실제 제어에 가까운 시뮬레이터를 제공
```
전문가 데이터: 한 작업에 대하여 높은 성공률을 가지는 정책을 기반으로 생성된 상태와 행동 데이터