# ANN+DQN 
온도 예측 ANN을 환경으로 하여 DQN 강화학습을 진행하는 것을 목표로한다.
## 보상함수

## 1. **핵심 설계 컨셉 요약**

* **실내 온도 추종 오차**와 **에너지 사용량**을 동시에 고려
* **보상은 두 부분:**

  1. **목표 온도 근접도(오차별 구간 보상)**
  2. **에너지 소비 패널티**

---

## 2. **상세 분석**

### **a. 입력 변수 해석**

* `tid1_curr`: 현재 실내온도
* `tid1_next`: 다음 스텝의 실내온도
* `tod`: 외기온도
* `tcon1, tcon2`: 두 실내기의 희망온도
* `frun1, frun2`: 두 실내기의 풍량(팬 세기)
* `on_off1, on_off2`: 두 실내기의 ON/OFF (1=켜짐, 0=꺼짐)

---

### **b. 유효 희망온도 및 풍량 계산**

```python
tcon1_eff = tcon1 if on_off1 else tid1_curr
tcon2_eff = tcon2 if on_off2 else tid1_curr
eff_frun1 = frun1 if on_off1 else 1
eff_frun2 = frun2 if on_off2 else 1
```

* **실내기가 OFF일 땐**:

  * 희망온도는 의미 없음 → 현재 실내온도로 고정
  * 풍량도 의미 없음 → 1(최소값)로 고정

---

### **c. 에너지 사용량 계산**

```python
energy = ((tid1_curr + tod - tcon1_eff) * eff_frun1 +
          (tid1_curr + tod - tcon2_eff) * eff_frun2)
```

* **핵심 아이디어:**

  * (실내온도 + 외기온도 - 희망온도) × 풍량 =
    **실내기가 더 많이 식히려고 할수록, 풍량이 클수록 에너지 사용이 커짐**
* **두 실내기의 소비량을 더함**

---

### **d. 목표 온도 오차별 보상 (Discrete Reward)**

```python
ref = self.ref_temp[self.step_idx+1]
err = abs(tid1_next - ref)
if err <= 0.5:
    temp_reward = 10
elif err <= 1.0:
    temp_reward = 5
elif err <= 1.5:
    temp_reward = 2
else:
    temp_reward = -5
```

* **목표 온도와 실제 온도의 오차(err) 크기에 따라 구간별 보상**

  * 오차 ≤ 0.5 → +10점 (아주 잘 맞췄을 때)
  * 오차 ≤ 1.0 → +5점 (거의 근접)
  * 오차 ≤ 1.5 → +2점 (그럭저럭)
  * 오차 > 1.5 → -5점 (실패/멀어짐)
* **실내온도를 목표에 더 가깝게 맞출수록 보상이 급격히 올라감**

---

### **e. 최종 보상 계산**

```python
return temp_reward - self.energy_weight * energy
```

* **목표 온도 달성 보상**에서
  **에너지 사용량(energy × 가중치)** 만큼 패널티를 부여
* `self.energy_weight`가 클수록 에너지 효율을 더 강하게 유도

---

## 3. **요약**

* **실내온도를 목표에 정확히 맞추면서**
* **불필요한 에너지 낭비(에어컨 세게/오래 돌리기)를 억제**
* **실내기 OFF 상태에서는 소비/보상이 자연스럽게 0에 가까움**

---

만약 좀 더 **정량적 분석**이 필요하면,평균 reward, 평균 energy 사용량 등의 로그를 남겨서 정책이 어디에 초점을 두는지 직접 확인해볼 수도 있음

실외기에 의해서 에너지 소모가 대부분 결정되는 특성을 반영시켜 off 시킨다고 해서 에너지 소모가 드라마틱하게 줄어드는 것이 아님을 반영할 수 있음

온도 추종 보상과 에너지 소모 패널티에 대한 보상 함수 설계는 추후 추가적인 보정 필요 (Emax 개념 등)

## ANN 모델 불러오기
```py
import torch
import torch.nn as nn
import joblib

# ANN 구조 재정의 (학습 시 사용했던 것과 동일해야 함)
class ANN(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, output_dim)
        )
    def forward(self, x):
        return self.net(x)

# 하이퍼파라미터
input_dim = 8  # Tid1, Tod, Frun1, Frun2, Tcon1, Tcon2, OnOff1, OnOff2
output_dim = 1  # Tid1_next

# 모델 로드
model = ANN(input_dim=input_dim, output_dim=output_dim)
model.load_state_dict(torch.load("aircon_tid1_predictor.pt"))
model.eval()

# 스케일러 로드
x_scaler = joblib.load("x_scaler.pkl")
y_scaler = joblib.load("y_scaler.pkl")
```

## Tod 시나리오 생성
오전 10시부터 오후 10시까지를 한 에피소드로 가정

20도에서 30도 사이가 되도록 설정

구간을 나눠 하락과 상승을 하도록 설정

실제 본 프로젝트에서는 다양한 날씨 데이터 활용하는게 좋음

## reference Tid 시나리오 생성

처음 두스텝 동안 25도로 떨어진 후 유지하는 트레젝토리 생성 - > 추후에 보다 현실적인 트레젝토리 정의 필요

## 강화학습
## DQN 구조 정의
```PY
import torch.nn as nn

class DQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )
    def forward(self, x):
        return self.net(x)
```

## 디스크리트 액션 스페이스 정의

```PY
# 예시: 2개 실내기(온도7×풍량5×온오프2)
action_space = []
for o1 in range(len(env.onoff_choices)):       # on_off1
    for o2 in range(len(env.onoff_choices)):   # on_off2
        if o1 == 0:
            t1_list = [0]  # 희망온도 0 (dummy, 의미 없음)
            f1_list = [0]  # 풍량 0
        else:
            t1_list = range(len(env.temp_choices))
            f1_list = range(len(env.frun_choices))
        if o2 == 0:
            t2_list = [0]
            f2_list = [0]
        else:
            t2_list = range(len(env.temp_choices))
            f2_list = range(len(env.frun_choices))
        for t1 in t1_list:
            for t2 in t2_list:
                for f1 in f1_list:
                    for f2 in f2_list:
                        action_space.append([t1, t2, f1, f2, o1, o2])
action_dim = len(action_space)


def action_index_to_vec(idx):
    return action_space[idx]
```
## 학습 설정
```PY
import random
from collections import deque
import torch.optim as optim
import numpy as np

# 경험버퍼
replay_buffer = deque(maxlen=20000)
batch_size = 64

# 에이전트 준비
state_dim = 8
dqn = DQN(state_dim, action_dim)
target_dqn = DQN(state_dim, action_dim)
target_dqn.load_state_dict(dqn.state_dict())
optimizer = optim.Adam(dqn.parameters(), lr=0.001)
gamma = 0.7

def store_exp(s, a, r, s_, done):
    replay_buffer.append((s, a, r, s_, done))

loss_history = []

def train():
    if len(replay_buffer) < batch_size:
        return None, None, None
    batch = random.sample(replay_buffer, batch_size)
    states, actions, rewards, next_states, dones = zip(*batch)
    states = torch.tensor(np.array(states), dtype=torch.float32)
    actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1)
    rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)
    next_states = torch.tensor(np.array(next_states), dtype=torch.float32)
    dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)

    q_values = dqn(states).gather(1, actions)
    next_q_values = target_dqn(next_states).max(1)[0].unsqueeze(1)
    target = rewards + gamma * next_q_values * (1 - dones)
    #scale = ((q_values.abs().mean() + target.abs().mean()) / 2) + 1e-6
    #loss = nn.L1Loss()(q_values, target.detach()) / scale
    loss = nn.MSELoss()(q_values, target.detach())
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    # 평균값 반환
    return q_values.mean().item(), target.mean().item(), loss.item()
```
## 학습 루프 (for문 max steps -1까지로 하는데 약간 야매임, 추후 수정 필요)
```PY
num_episodes = 1000
epsilon = 1.0
epsilon_min = 0.05
epsilon_decay = 0.995
target_update_period = 10

for ep in range(num_episodes):
    state = env.reset()
    total_reward = 0
    mean_q, mean_target, mean_loss = None, None, None  # 매 에피소드 마지막 기록
    for t in range(env.max_steps-1):
        # Epsilon-greedy action 선택
        if np.random.rand() < epsilon:
            action_idx = np.random.randint(action_dim)
        else:
            with torch.no_grad():
                q = dqn(torch.tensor(state, dtype=torch.float32).unsqueeze(0)).squeeze()
                action_idx = int(torch.argmax(q))
        action_vec = action_index_to_vec(action_idx)
        next_state, reward, done, _ = env.step(action_vec)
        store_exp(state, action_idx, reward, next_state, done)
        q_val, target_val, loss_val = train()
        # 마지막 스텝의 값만 기록
        if q_val is not None:
            mean_q, mean_target, mean_loss = q_val, target_val, loss_val
        state = next_state
        total_reward += reward
        if done:
            break
    if ep % target_update_period == 0:
        target_dqn.load_state_dict(dqn.state_dict())
    epsilon = max(epsilon * epsilon_decay, epsilon_min)
    print(f"EP {ep:04d} | Reward: {total_reward:.2f} | Eps: {epsilon:.3f} | Q: {mean_q:.4f} | target: {mean_target:.4f} | loss: {mean_loss:.4f}|")
```


## 레퍼런스 온도 궤적 반영된 환경설정
```PY
import numpy as np
import torch
import torch.nn as nn
import joblib

# ---- 외기온도 시나리오 생성 ----
def generate_realistic_tod_scenario(seed=None):
    if seed is not None:
        np.random.seed(seed)
    total_steps = 72
    morning_steps, afternoon_steps, evening_steps = 18, 18, 36
    morning = np.linspace(21, 26, morning_steps) + np.random.normal(0, 0.3, morning_steps)
    afternoon = np.ones(afternoon_steps) * (26 + np.random.normal(0.5, 0.3)) + np.random.normal(0, 0.3, afternoon_steps)
    evening = np.linspace(29, 22, evening_steps) + np.random.normal(0, 0.4, evening_steps)
    tod = np.clip(np.concatenate([morning, afternoon, evening]), 20, 30)
    return tod

# 레퍼런스 온도 궤적 생성
def generate_reference_temp_trajectory(start_temp, total_steps=72, fall_steps=4, target_temp=23):
    tau = 2  # 감쇠 속도 (더 빠르게 도달하고 싶으면 더 작게 조정)
    t = np.arange(fall_steps)
    fall = target_temp + (start_temp - target_temp) * np.exp(-t / tau)
    steady = np.ones(total_steps - fall_steps) * target_temp
    ref_temp = np.concatenate([fall, steady])
    return ref_temp

# ---- ANN 모델 정의 및 불러오기 ----
class ANN(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, output_dim)
        )
    def forward(self, x):
        return self.net(x)

x_scaler = joblib.load('x_scaler.pkl')
y_scaler = joblib.load('y_scaler.pkl')
ann_model = ANN(input_dim=14, output_dim=1)
ann_model.load_state_dict(torch.load('aircon_tid1_predictor.pt', map_location='cpu'))
ann_model.eval()

# ---- 강화학습 환경 ----
class HVACEnv:
    def __init__(self, energy_weight=0.05, seed=None):
        self.energy_weight = energy_weight
        self.tod_scenario = generate_realistic_tod_scenario(seed)
        self.max_steps = len(self.tod_scenario)
        self.temp_choices = np.arange(20, 27)
        self.frun_choices = np.arange(1, 6)
        self.onoff_choices = np.array([0, 1])
        self.reset()
    
    def reset(self):
        self.step_idx = 0
        self.tid1 = np.random.uniform(27, 31)
        self.ref_temp = generate_reference_temp_trajectory(self.tid1, total_steps=self.max_steps+1)
        # prev_ctrl: frun1_prev, frun2_prev, tcon1_prev, tcon2_prev, on_off1_prev, on_off2_prev (모두 0)
        self.prev_ctrl = np.zeros(6, dtype=np.float32)
        return self._get_obs()
    
    def _get_obs(self):
        tod = self.tod_scenario[self.step_idx]
        # state 순서: Tid1, Tod, frun1_prev, frun2_prev, tcon1_prev, tcon2_prev, on_off1_prev, on_off2_prev
        state = np.concatenate([[self.tid1, tod], self.prev_ctrl])
        return state.astype(np.float32)
    
    def compute_reward(self, tid1_curr, tid1_next, tod,
                       tcon1, tcon2, frun1, frun2, on_off1, on_off2):
        eff_frun1 = frun1 if on_off1 else 1
        eff_frun2 = frun2 if on_off2 else 1
        tcon1_eff = tcon1 if on_off1 else tid1_curr
        tcon2_eff = tcon2 if on_off2 else tid1_curr
        energy = ((tid1_curr + tod - tcon1_eff) * frun1 +
                  (tid1_curr + tod - tcon2_eff) * frun2)
        ref = self.ref_temp[self.step_idx+1]
        err = abs(tid1_next - ref)
        # 오차 구간별 보상 설계
        if err <= 0.5:
            temp_reward = 10
        elif err <= 1.0:
            temp_reward = 5
        elif err <= 1.5:
            temp_reward = 2
        else:
            temp_reward = -5
        # 에너지 패널티(기존 방식)
        eff_frun1 = frun1 if on_off1 else 1
        eff_frun2 = frun2 if on_off2 else 1
        tcon1_eff = tcon1 if on_off1 else tid1_curr
        tcon2_eff = tcon2 if on_off2 else tid1_curr
        energy = ((tid1_curr + tod - tcon1_eff) * eff_frun1 +
                  (tid1_curr + tod - tcon2_eff) * eff_frun2)
        return temp_reward - self.energy_weight * energy
    
    def step(self, action_idx):
        # action_idx: [tcon1_idx, tcon2_idx, frun1_idx, frun2_idx, on_off1, on_off2]
        on_off1 = self.onoff_choices[action_idx[4]]
        on_off2 = self.onoff_choices[action_idx[5]]
        tcon1 = self.temp_choices[action_idx[0]] if on_off1 == 1 else 0
        tcon2 = self.temp_choices[action_idx[1]] if on_off2 == 1 else 0
        frun1 = self.frun_choices[action_idx[2]] if on_off1 == 1 else 0
        frun2 = self.frun_choices[action_idx[3]] if on_off2 == 1 else 0

        tid1_curr = self.tid1
        tod = self.tod_scenario[self.step_idx]
        # ---- ANN 입력 순서 반영 ----
        # [Tid1, Tod, frun1_prev, frun2_prev, tcon1_prev, tcon2_prev, on_off1_prev, on_off2_prev,
        #  frun1, frun2, tcon1, tcon2, on_off1, on_off2]
        x_input = np.array([
            tid1_curr, tod,
            self.prev_ctrl[0], self.prev_ctrl[1], self.prev_ctrl[2], self.prev_ctrl[3], self.prev_ctrl[4], self.prev_ctrl[5],
            frun1, frun2, tcon1, tcon2, on_off1, on_off2
        ]).reshape(1, -1)
        x_scaled = x_scaler.transform(x_input)
        with torch.no_grad():
            tid1_next_scaled = ann_model(torch.tensor(x_scaled, dtype=torch.float32)).numpy()
        tid1_next = y_scaler.inverse_transform(tid1_next_scaled)[0, 0]
        reward = self.compute_reward(
            tid1_curr, tid1_next, tod,
            tcon1, tcon2, frun1, frun2, on_off1, on_off2
        )
        # prev_ctrl도 ANN 인풋순서에 맞게 최신 값으로 교체
        self.tid1 = tid1_next
        self.prev_ctrl = np.array([frun1, frun2, tcon1, tcon2, on_off1, on_off2], dtype=np.float32)
        self.step_idx += 1
        done = self.step_idx >= self.max_steps
        # 여기서 obs를 안전하게 반환
        obs = self._get_obs() if not done else np.zeros_like(self._get_obs())
        return obs, reward, done, {}
```

## 환경 테스트
```py
# 환경 생성 및 초기화
env = HVACEnv(energy_weight=0.05)
obs = env.reset()


# 환경 한 스텝 진행
action_idx = [
    np.random.randint(len(env.temp_choices)),
    np.random.randint(len(env.temp_choices)),
    np.random.randint(len(env.frun_choices)),
    np.random.randint(len(env.frun_choices)),
    np.random.randint(len(env.onoff_choices)),
    np.random.randint(len(env.onoff_choices)),
]
next_obs, reward, done, _ = env.step(action_idx)
print("초기 상태:", obs)
print("다음 상태:", next_obs)
print(f"초기 레퍼런스 온도(ref_temp[0]): {env.ref_temp[0]:.2f} °C")
print(f"다음 레퍼런스 온도(ref_temp[1]): {env.ref_temp[1]:.2f} °C")
print("보상:", reward)

# 전체 레퍼런스 궤적 시각화
import matplotlib.pyplot as plt
plt.figure(figsize=(8, 4))
plt.plot(env.ref_temp, marker='o')
plt.title(f"Reference Temperature Trajectory (start={env.tid1:.2f}°C → 23°C)")
plt.xlabel("Step")
plt.ylabel("Reference Temp (°C)")
plt.grid(True)
plt.show()
```
## 랜덤 액션으로 확인한 온도 궤적과 보상 그래프
```py
import matplotlib.pyplot as plt
import numpy as np

# (action_space, action_index_to_vec는 이미 정의되어 있다고 가정)
def decode_action_vec(vec, env):
    t1_idx, t2_idx, f1_idx, f2_idx, o1, o2 = vec
    t1 = 0 if o1 == 0 else int(env.temp_choices[t1_idx])
    f1 = 0 if o1 == 0 else int(env.frun_choices[f1_idx])
    t2 = 0 if o2 == 0 else int(env.temp_choices[t2_idx])
    f2 = 0 if o2 == 0 else int(env.frun_choices[f2_idx])
    return [t1, t2, f1, f2, int(o1), int(o2)]

env = HVACEnv(energy_weight=0.05)
state = env.reset()

tid1_traj = [env.tid1]
ref_traj = [env.ref_temp[0]]
reward_traj = []
action_idx_traj = []
action_vec_traj = []

print("Step | Action_idx | Action_vec | Reward | Tid1 | Ref")
print("------------------------------------------------------")

for t in range(env.max_steps - 1):
    action_idx = np.random.randint(len(action_space))
    action_vec = action_index_to_vec(action_idx)
    # 변환해서 실제 값으로 출력
    human_action = decode_action_vec(action_vec, env)
    next_state, reward, done, _ = env.step(action_vec)
    tid1_traj.append(env.tid1)
    ref_traj.append(env.ref_temp[env.step_idx])
    reward_traj.append(reward)
    action_idx_traj.append(action_idx)
    action_vec_traj.append(action_vec)

    print(f"{t+1:>4} | {action_idx:>10} | {human_action} | {reward:>6.2f} | {env.tid1:>5.2f} | {env.ref_temp[env.step_idx]:>5.2f}")

    if done:
        break

steps = np.arange(len(tid1_traj))

plt.figure(figsize=(10, 5))
plt.subplot(2, 1, 1)
plt.plot(steps, tid1_traj, label='Tid1', marker='o')
plt.plot(steps, ref_traj, '--', label='Reference Temp', marker='x')
plt.title('Real vs. Reference')
plt.xlabel('Step')
plt.ylabel('Temperature (°C)')
plt.legend()
plt.grid(True)

plt.subplot(2, 1, 2)
plt.plot(steps[1:], reward_traj, color='purple', label='Reward', marker='s')
plt.title('Reward by step')
plt.xlabel('Step')
plt.ylabel('Reward')
plt.grid(True)
plt.tight_layout()
plt.show()

# 총 reward 출력
total_reward = np.sum(reward_traj)
print(f"Total Reward: {total_reward:.2f}")
```

## DQN 정책으로 샘플링한 액션으로 확인한 온도 궤적과 보상 그래프
```PY
import matplotlib.pyplot as plt
import torch
import numpy as np

env = HVACEnv(energy_weight=0.05)
state = env.reset()

tid1_traj = [env.tid1]
ref_traj = [env.ref_temp[0]]
reward_traj = []
action_traj = []

total_reward = 0

for t in range(env.max_steps - 1):
    # DQN 정책으로 액션 선택 (탐험 없이 Q값이 최대인 액션)
    with torch.no_grad():
        q_values = dqn(torch.tensor(state, dtype=torch.float32).unsqueeze(0)).squeeze()
        action_idx = int(torch.argmax(q_values))
    action_vec = action_index_to_vec(action_idx)
    action_traj.append(action_vec)
    next_state, reward, done, _ = env.step(action_vec)
    tid1_traj.append(env.tid1)
    ref_traj.append(env.ref_temp[env.step_idx])
    reward_traj.append(reward)
    state = next_state
    total_reward += reward
    # 출력: 각 스텝별 액션/보상/온도
    # 온도, 풍량, on/off 해석: action_vec = [t1_idx, t2_idx, f1_idx, f2_idx, o1, o2]
    # 예시: temp_choices[action_vec[0]] 등으로 변환
    t1 = env.temp_choices[action_vec[0]] if action_vec[4] == 1 else '-'
    t2 = env.temp_choices[action_vec[1]] if action_vec[5] == 1 else '-'
    f1 = env.frun_choices[action_vec[2]] if action_vec[4] == 1 else '-'
    f2 = env.frun_choices[action_vec[3]] if action_vec[5] == 1 else '-'
    o1 = action_vec[4]
    o2 = action_vec[5]
    print(f"{t+1:3d} | Action: [{t1},{t2},{f1},{f2},{o1},{o2}] | Reward: {reward:6.2f} | Tid1: {env.tid1:5.2f} | Ref: {env.ref_temp[env.step_idx]:5.2f}")
    if done:
        break

print("="*60)

steps = np.arange(len(tid1_traj))

plt.figure(figsize=(10, 5))
plt.subplot(2, 1, 1)
plt.plot(steps, tid1_traj, label='Tid1', marker='o')
plt.plot(steps, ref_traj, '--', label='Reference Temp', marker='x')
plt.title('real based DQN policy vs. Reference')
plt.xlabel('Step')
plt.ylabel('Temperature (°C)')
plt.legend()
plt.grid(True)

plt.subplot(2, 1, 2)
plt.plot(steps[1:], reward_traj, color='green', label='Reward', marker='s')
plt.title('reward by step based DQN policy ')
plt.xlabel('Step')
plt.ylabel('Reward')
plt.grid(True)
plt.tight_layout()
plt.show()

print(f"Total Reward: {total_reward:.2f}")
```
### 저장된 pt 불러와서 평가에 사용
```py
dqn = DQN(state_dim, action_dim)
dqn.load_state_dict(torch.load("dqn_policy_2.pt", map_location='cpu'))  # or 'cuda' if you use GPU
dqn.eval()  # 추론(테스트) 모드로 전환 (dropout 등 비활성화)
```

## 핵심 개념 정리

64개의 (s, a, r, s') 쌍을 리플라이 버퍼에서 랜덤으로 뽑아두고 스테이트들에 대해 실제 행한 액션과 매칭시켜서 q값을 모두 구하고(64개) 그 64개의 평균을 구한게 mean_q

64개의 모든 다음 스테이트(s')들에 대해서 q값이 가장 크게하는 액션a'에 대해 64개의 next q를 구하고 64개의 target을 구해둔 뒤, 그 64개의 평균을 구한게 mean_target

각 64개의 쌍을 맞춘 (s, a, r, s')에 대해 대응되는  각각 Q와 타겟의 오차의 제곱 평균이 LOSS  


### Q(s, a)는 감쇠계수 적용된 무한합(미래 기대 누적 보상)에 수렴하게 설계됨. (벨만 이큐에이션의 수학적 특성)

Q값이 크더라도 업데이트 변화율이 작으면 “수렴했다”고 볼 수 있음.


### 정책 Q 에트워크와 타깃 Q네트워크 구분

정책 Q는 매스텝 마다 업데이트, 타깃 계산에 쓰는 Q는 10개의 에피소드마다 업데이트

정책 Q로 예측한 Q, 타깃 Q로 계산된 타깃, 로스를 보도록 코드 설계됨



### 감쇠계수, 보상 범위 매우매우매우 중요

보상범위 조절: 에너지 가중치 0.01 -> 0.05
감쇠계수 조절: 0.99 - > 0.7

변경전: Q밸류 계속해서 커짐, 로스도 따라서 발산
변경후: Q밸류 20단위로 안정적 수렴, 로스도 낮은 범위에서 머무름
