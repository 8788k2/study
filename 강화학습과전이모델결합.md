# ANN+DQN 
온도 예측 ANN을 환경으로 하여 DQN 강화학습을 진행하는 것을 목표로한다.
## 보상함수
에너지 소모에 관한 모델 기반으로 에너지 소모에 관한 패널티를 선정하였다.

$$
\text{Reward} = T_{\text{id1,current}} - T_{\text{id1,next}} 
- C \cdot \left[
(T_{\text{id1,current}} + T_{\text{od}} - T_{\text{con1}}) \cdot \text{frun1}
+ (T_{\text{id1,current}} + T_{\text{od}} - T_{\text{con2}}) \cdot \text{frun2}
\right]
$$
단 실내기가 off, 즉 on/off=0이면,
$$
T_{\text{con1}} := T_{\text{id1,current}} \\
T_{\text{con2}} := T_{\text{id1,current}}
$$


$T_{\text{id1}} - T_{\text{con}} = 0$이 되므로 자연스럽게 소모 에너지량은 아래와 같이 남게 됨

$$
T_{\text{od}} \times \text{frun} \times C
$$

실외기에 의해서 에너지 소모가 대부분 결정되는 특성을 반영시켜 off 시킨다고 해서 에너지 소모가 드라마틱하게 줄어드는 것이 아님을 반영할 수 있음

온도 추종 보상과 에너지 소모 패널티에 대한 보상 함수 설계는 추후 추가적인 보정 필요 (Emax 개념 등)

## ANN 모델 불러오기
```py
import torch
import torch.nn as nn
import joblib

# ANN 구조 재정의 (학습 시 사용했던 것과 동일해야 함)
class ANN(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, output_dim)
        )
    def forward(self, x):
        return self.net(x)

# 하이퍼파라미터
input_dim = 8  # Tid1, Tod, Frun1, Frun2, Tcon1, Tcon2, OnOff1, OnOff2
output_dim = 1  # Tid1_next

# 모델 로드
model = ANN(input_dim=input_dim, output_dim=output_dim)
model.load_state_dict(torch.load("aircon_tid1_predictor.pt"))
model.eval()

# 스케일러 로드
x_scaler = joblib.load("x_scaler.pkl")
y_scaler = joblib.load("y_scaler.pkl")
```

## Tod 시나리오 생성
오전 10시부터 오후 10시까지를 한 에피소드로 가정

20도에서 30도 사이가 되도록 설정

구간을 나눠 하락과 상승을 하도록 설정

실제 본 프로젝트에서는 다양한 날씨 데이터 활용하는게 좋음

## reference Tid 시나리오 생성

``

## 강화학습
## DQN 구조 정의
```PY
import torch.nn as nn

class DQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )
    def forward(self, x):
        return self.net(x)
```

## 디스크리트 액션 스페이스 정의

```PY
# 예시: 2개 실내기(온도7×풍량5×온오프2), 조합 7*5*2*7*5*2=4900개
action_space = []
for o1 in range(len(env.onoff_choices)):       # on_off1
    for o2 in range(len(env.onoff_choices)):   # on_off2
        if o1 == 0:
            t1_list = [0]  # 희망온도 0 (dummy, 의미 없음)
            f1_list = [0]  # 풍량 0
        else:
            t1_list = range(len(env.temp_choices))
            f1_list = range(len(env.frun_choices))
        if o2 == 0:
            t2_list = [0]
            f2_list = [0]
        else:
            t2_list = range(len(env.temp_choices))
            f2_list = range(len(env.frun_choices))
        for t1 in t1_list:
            for t2 in t2_list:
                for f1 in f1_list:
                    for f2 in f2_list:
                        action_space.append([t1, t2, f1, f2, o1, o2])
action_dim = len(action_space)


def action_index_to_vec(idx):
    return action_space[idx]
```
## 리플라이 버퍼 설정
```PY
import random
from collections import deque
import torch.optim as optim
import numpy as np

# 경험버퍼
replay_buffer = deque(maxlen=20000)
batch_size = 64

# 에이전트 준비
state_dim = 8
dqn = DQN(state_dim, action_dim)
target_dqn = DQN(state_dim, action_dim)
target_dqn.load_state_dict(dqn.state_dict())
optimizer = optim.Adam(dqn.parameters(), lr=0.001)
gamma = 0.99

def store_exp(s, a, r, s_, done):
    replay_buffer.append((s, a, r, s_, done))

def train():
    if len(replay_buffer) < batch_size:
        return
    batch = random.sample(replay_buffer, batch_size)
    states, actions, rewards, next_states, dones = zip(*batch)
    states = torch.tensor(np.array(states), dtype=torch.float32)
    actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1)
    rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)
    next_states = torch.tensor(np.array(next_states), dtype=torch.float32)
    dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)

    q_values = dqn(states).gather(1, actions)
    next_q_values = target_dqn(next_states).max(1)[0].unsqueeze(1)
    target = rewards + gamma * next_q_values * (1 - dones)
    loss = nn.MSELoss()(q_values, target.detach())
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```
## 학습 루프 (for문 max steps -1까지로 하는데 약간 야매임, 추후 수정 필요)
```PY
num_episodes = 1000
epsilon = 1.0
epsilon_min = 0.05
epsilon_decay = 0.995
target_update_period = 10

for ep in range(num_episodes):
    state = env.reset()
    total_reward = 0
    for t in range(env.max_steps-1):
        # Epsilon-greedy action 선택
        if np.random.rand() < epsilon:
            action_idx = np.random.randint(action_dim)
        else:
            with torch.no_grad():
                q = dqn(torch.tensor(state, dtype=torch.float32).unsqueeze(0)).squeeze()
                action_idx = int(torch.argmax(q))
        action_vec = action_index_to_vec(action_idx)
        next_state, reward, done, _ = env.step(action_vec)
        # 아래에서 done이면 break 전에 경험 저장/학습만 수행
        store_exp(state, action_idx, reward, next_state, done)
        train()
        state = next_state
        total_reward += reward
        if done:
            break  # done이 True면 IndexError 없이 즉시 루프 탈출
    if ep % target_update_period == 0:
        target_dqn.load_state_dict(dqn.state_dict())
    epsilon = max(epsilon * epsilon_decay, epsilon_min)
    print(f"EP {ep:04d} | Reward: {total_reward:.2f} | Eps: {epsilon:.3f}")
```


## 레퍼런스 온도 궤적 반영된 환경설정
```PY
import numpy as np
import torch
import torch.nn as nn
import joblib

# ---- 외기온도 시나리오 생성 ----
def generate_realistic_tod_scenario(seed=None):
    if seed is not None:
        np.random.seed(seed)
    total_steps = 72
    morning_steps, afternoon_steps, evening_steps = 18, 18, 36
    morning = np.linspace(21, 26, morning_steps) + np.random.normal(0, 0.3, morning_steps)
    afternoon = np.ones(afternoon_steps) * (26 + np.random.normal(0.5, 0.3)) + np.random.normal(0, 0.3, afternoon_steps)
    evening = np.linspace(29, 22, evening_steps) + np.random.normal(0, 0.4, evening_steps)
    tod = np.clip(np.concatenate([morning, afternoon, evening]), 20, 30)
    return tod

# 레퍼런스 온도 궤적 생성
def generate_reference_temp_trajectory(start_temp, total_steps=72, fall_steps=4, target_temp=23):
    tau = 2  # 감쇠 속도 (더 빠르게 도달하고 싶으면 더 작게 조정)
    t = np.arange(fall_steps)
    fall = target_temp + (start_temp - target_temp) * np.exp(-t / tau)
    steady = np.ones(total_steps - fall_steps) * target_temp
    ref_temp = np.concatenate([fall, steady])
    return ref_temp

# ---- ANN 모델 정의 및 불러오기 ----
class ANN(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, output_dim)
        )
    def forward(self, x):
        return self.net(x)

x_scaler = joblib.load('x_scaler.pkl')
y_scaler = joblib.load('y_scaler.pkl')
ann_model = ANN(input_dim=14, output_dim=1)
ann_model.load_state_dict(torch.load('aircon_tid1_predictor.pt', map_location='cpu'))
ann_model.eval()

# ---- 강화학습 환경 ----
class HVACEnv:
    def __init__(self, energy_weight=0.01, seed=None):
        self.energy_weight = energy_weight
        self.tod_scenario = generate_realistic_tod_scenario(seed)
        self.max_steps = len(self.tod_scenario)
        self.temp_choices = np.arange(20, 27)
        self.frun_choices = np.arange(1, 6)
        self.onoff_choices = np.array([0, 1])
        self.reset()
    
    def reset(self):
        self.step_idx = 0
        self.tid1 = np.random.uniform(27, 31)
        self.ref_temp = generate_reference_temp_trajectory(self.tid1, total_steps=self.max_steps+1)
        # prev_ctrl: frun1_prev, frun2_prev, tcon1_prev, tcon2_prev, on_off1_prev, on_off2_prev (모두 0)
        self.prev_ctrl = np.zeros(6, dtype=np.float32)
        return self._get_obs()
    
    def _get_obs(self):
        tod = self.tod_scenario[self.step_idx]
        # state 순서: Tid1, Tod, frun1_prev, frun2_prev, tcon1_prev, tcon2_prev, on_off1_prev, on_off2_prev
        state = np.concatenate([[self.tid1, tod], self.prev_ctrl])
        return state.astype(np.float32)
    
    def compute_reward(self, tid1_curr, tid1_next, tod,
                       tcon1, tcon2, frun1, frun2, on_off1, on_off2):
        eff_frun1 = frun1 if on_off1 else 1
        eff_frun2 = frun2 if on_off2 else 1
        tcon1_eff = tcon1 if on_off1 else tid1_curr
        tcon2_eff = tcon2 if on_off2 else tid1_curr
        energy = ((tid1_curr + tod - tcon1_eff) * frun1 +
                  (tid1_curr + tod - tcon2_eff) * frun2)
        ref = self.ref_temp[self.step_idx+1]
        err = abs(tid1_next - ref)
        # 오차 구간별 보상 설계
        if err <= 0.5:
            temp_reward = 10
        elif err <= 1.0:
            temp_reward = 5
        elif err <= 1.5:
            temp_reward = 2
        else:
            temp_reward = -5
        # 에너지 패널티(기존 방식)
        eff_frun1 = frun1 if on_off1 else 1
        eff_frun2 = frun2 if on_off2 else 1
        tcon1_eff = tcon1 if on_off1 else tid1_curr
        tcon2_eff = tcon2 if on_off2 else tid1_curr
        energy = ((tid1_curr + tod - tcon1_eff) * eff_frun1 +
                  (tid1_curr + tod - tcon2_eff) * eff_frun2)
        return temp_reward - self.energy_weight * energy
    
    def step(self, action_idx):
        # action_idx: [tcon1_idx, tcon2_idx, frun1_idx, frun2_idx, on_off1, on_off2]
        on_off1 = self.onoff_choices[action_idx[4]]
        on_off2 = self.onoff_choices[action_idx[5]]
        tcon1 = self.temp_choices[action_idx[0]] if on_off1 == 1 else 0
        tcon2 = self.temp_choices[action_idx[1]] if on_off2 == 1 else 0
        frun1 = self.frun_choices[action_idx[2]] if on_off1 == 1 else 0
        frun2 = self.frun_choices[action_idx[3]] if on_off2 == 1 else 0

        tid1_curr = self.tid1
        tod = self.tod_scenario[self.step_idx]
        # ---- ANN 입력 순서 반영 ----
        # [Tid1, Tod, frun1_prev, frun2_prev, tcon1_prev, tcon2_prev, on_off1_prev, on_off2_prev,
        #  frun1, frun2, tcon1, tcon2, on_off1, on_off2]
        x_input = np.array([
            tid1_curr, tod,
            self.prev_ctrl[0], self.prev_ctrl[1], self.prev_ctrl[2], self.prev_ctrl[3], self.prev_ctrl[4], self.prev_ctrl[5],
            frun1, frun2, tcon1, tcon2, on_off1, on_off2
        ]).reshape(1, -1)
        x_scaled = x_scaler.transform(x_input)
        with torch.no_grad():
            tid1_next_scaled = ann_model(torch.tensor(x_scaled, dtype=torch.float32)).numpy()
        tid1_next = y_scaler.inverse_transform(tid1_next_scaled)[0, 0]
        reward = self.compute_reward(
            tid1_curr, tid1_next, tod,
            tcon1, tcon2, frun1, frun2, on_off1, on_off2
        )
        # prev_ctrl도 ANN 인풋순서에 맞게 최신 값으로 교체
        self.tid1 = tid1_next
        self.prev_ctrl = np.array([frun1, frun2, tcon1, tcon2, on_off1, on_off2], dtype=np.float32)
        self.step_idx += 1
        done = self.step_idx >= self.max_steps
        # 여기서 obs를 안전하게 반환
        obs = self._get_obs() if not done else np.zeros_like(self._get_obs())
        return obs, reward, done, {}
```

## 환경 테스트
```py
# 환경 생성 및 초기화
env = HVACEnv(energy_weight=0.01)
obs = env.reset()


# 환경 한 스텝 진행
action_idx = [
    np.random.randint(len(env.temp_choices)),
    np.random.randint(len(env.temp_choices)),
    np.random.randint(len(env.frun_choices)),
    np.random.randint(len(env.frun_choices)),
    np.random.randint(len(env.onoff_choices)),
    np.random.randint(len(env.onoff_choices)),
]
next_obs, reward, done, _ = env.step(action_idx)
print("초기 상태:", obs)
print("다음 상태:", next_obs)
print(f"초기 레퍼런스 온도(ref_temp[0]): {env.ref_temp[0]:.2f} °C")
print(f"다음 레퍼런스 온도(ref_temp[1]): {env.ref_temp[1]:.2f} °C")
print("보상:", reward)

# 전체 레퍼런스 궤적 시각화
import matplotlib.pyplot as plt
plt.figure(figsize=(8, 4))
plt.plot(env.ref_temp, marker='o')
plt.title(f"Reference Temperature Trajectory (start={env.tid1:.2f}°C → 23°C)")
plt.xlabel("Step")
plt.ylabel("Reference Temp (°C)")
plt.grid(True)
plt.show()
```
## 랜덤 액션으로 확인한 온도 궤적과 보상 그래프
```py
import matplotlib.pyplot as plt

env = HVACEnv(energy_weight=0.01)
state = env.reset()

tid1_traj = [env.tid1]
ref_traj = [env.ref_temp[0]]
reward_traj = []

for t in range(env.max_steps - 1):
    # 임의의 랜덤 액션 샘플링
    action_idx = [
        np.random.randint(len(env.temp_choices)),
        np.random.randint(len(env.temp_choices)),
        np.random.randint(len(env.frun_choices)),
        np.random.randint(len(env.frun_choices)),
        np.random.randint(len(env.onoff_choices)),
        np.random.randint(len(env.onoff_choices)),
    ]
    next_state, reward, done, _ = env.step(action_idx)
    tid1_traj.append(env.tid1)
    ref_traj.append(env.ref_temp[env.step_idx])
    reward_traj.append(reward)
    if done:
        break

steps = np.arange(len(tid1_traj))

plt.figure(figsize=(10, 5))
plt.subplot(2, 1, 1)
plt.plot(steps, tid1_traj, label='Tid1', marker='o')
plt.plot(steps, ref_traj, '--', label='Reference Temp', marker='x')
plt.title('real vs. Reference')
plt.xlabel('Step')
plt.ylabel('Temperature (°C)')
plt.legend()
plt.grid(True)

plt.subplot(2, 1, 2)
plt.plot(steps[1:], reward_traj, color='purple', label='Reward', marker='s')
plt.title('Reward by step')
plt.xlabel('Step')
plt.ylabel('Reward')
plt.grid(True)
plt.tight_layout()
plt.show()
```

## DQN 정책으로 샘플링한 액션으로 확인한 온도 궤적과 보상 그래프
```PY
import matplotlib.pyplot as plt
import torch
import numpy as np

env = HVACEnv(energy_weight=0.01)
state = env.reset()

tid1_traj = [env.tid1]
ref_traj = [env.ref_temp[0]]
reward_traj = []

for t in range(env.max_steps - 1):
    # DQN 정책으로 액션 선택 (탐험 없이 Q값이 최대인 액션)
    with torch.no_grad():
        q_values = dqn(torch.tensor(state, dtype=torch.float32).unsqueeze(0)).squeeze()
        action_idx = int(torch.argmax(q_values))
    action_vec = action_index_to_vec(action_idx)
    next_state, reward, done, _ = env.step(action_vec)
    tid1_traj.append(env.tid1)
    ref_traj.append(env.ref_temp[env.step_idx])
    reward_traj.append(reward)
    state = next_state
    if done:
        break

steps = np.arange(len(tid1_traj))

plt.figure(figsize=(10, 5))
plt.subplot(2, 1, 1)
plt.plot(steps, tid1_traj, label='Tid1', marker='o')
plt.plot(steps, ref_traj, '--', label='Reference Temp', marker='x')
plt.title('real based DQN policy vs. Reference')
plt.xlabel('Step')
plt.ylabel('Temperature (°C)')
plt.legend()
plt.grid(True)

plt.subplot(2, 1, 2)
plt.plot(steps[1:], reward_traj, color='green', label='Reward', marker='s')
plt.title('reward by step based DQN policy ')
plt.xlabel('Step')
plt.ylabel('Reward')
plt.grid(True)
plt.tight_layout()
plt.show()
```